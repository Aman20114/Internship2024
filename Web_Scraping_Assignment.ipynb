{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5eb48db-a5ca-4887-97be-b0d50a8481fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2219c492-90e2-42f1-9801-1e44a938e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff109871-4f9a-4e8e-8eec-77505241fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name        Rating    Year\n",
      "0                     Ship of Theseus  \\n\\n\\n\\n\\n\\n  (2012)\n",
      "1                              Iruvar  \\n\\n\\n\\n\\n\\n  (1997)\n",
      "2                     Kaagaz Ke Phool  \\n\\n\\n\\n\\n\\n  (1959)\n",
      "3   Lagaan: Once Upon a Time in India  \\n\\n\\n\\n\\n\\n  (2001)\n",
      "4                     Pather Panchali  \\n\\n\\n\\n\\n\\n  (1955)\n",
      "..                                ...           ...     ...\n",
      "95                        Apur Sansar  \\n\\n\\n\\n\\n\\n  (1959)\n",
      "96                        Kanchivaram  \\n\\n\\n\\n\\n\\n  (2008)\n",
      "97                    Monsoon Wedding  \\n\\n\\n\\n\\n\\n  (2001)\n",
      "98                              Black  \\n\\n\\n\\n\\n\\n  (2005)\n",
      "99                            Deewaar  \\n\\n\\n\\n\\n\\n  (1975)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "page = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "r = requests.get(page)\n",
    "\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# movies = []\n",
    "movies_list = soup.find_all('div', class_ = 'lister-item-content')\n",
    "\n",
    "for i in movies_list:\n",
    "    name = i.h3.a.text\n",
    "    rating = i.find('div', class_='ipl-rating-star small')\n",
    "    year = i.find('span', class_ = \"lister-item-year text-muted unbold\")\n",
    "    \n",
    "    movies.append({'Name': name, 'Rating': rating, 'Year': year})\n",
    "\n",
    "df = pd.DataFrame(movies)\n",
    "\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa2baebc-486f-45cf-938d-a772d0d8a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Product Name: <div class=\"product-title\">\n",
      "<a :href=\"getUrl(item.handle)\" @click=\"prodUrl(item.title)\">\n",
      "<span>${ item.title }$</span>\n",
      "</a>\n",
      "</div>\n",
      "Price: None\n",
      "Discount: None\n"
     ]
    }
   ],
   "source": [
    "url = 'https://peachmode.com/search?q=bags'\n",
    "\n",
    "r = requests.get(url)\n",
    "print(r)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "product_containers = soup.find_all('div', class_='st-product st-col-xs-6 st-col-sm-3 st-col-md-3')\n",
    "    \n",
    "for product in product_containers:\n",
    "    name = product.find('div', class_='product-title')\n",
    "    price = product.find('span', class_='price st-money money done')\n",
    "    discount_element = product.find('span', class_='product-prices')\n",
    "print(\"Product Name:\", name)\n",
    "print(\"Price:\", price)\n",
    "print(\"Discount:\", discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c23977dc-61f5-4f2b-ac7f-12cef75d41ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Top 10 ODI Teams:\n",
      "<Response [200]>\n",
      "Top 10 ODI Teams:\n",
      "<Response [200]>\n",
      "Top 10 ODI Bowlers:\n"
     ]
    }
   ],
   "source": [
    "url_teams = 'https://icc-cricket.com'\n",
    "res = requests.get(url_teams)\n",
    "print(r)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "teams = []\n",
    "for team_row in soup.find_all(\"div\", class_ = 'si-table-section'):\n",
    "    name = team_row.find('div', class_ = 'si-table-data si-team')\n",
    "    matches = team_row.find_all('div', class_ = \"si-table-data si-matches\")\n",
    "    points = team_row.find_all('div' ,class_ = 'si-table-data si-pts')\n",
    "    rating = team_row.find_all('div',class_ = \"si-table-data si-rating\")\n",
    "    teams.append({'name': name, 'matches': matches, 'points': points, 'rating': rating})\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "for team in teams:\n",
    "    print(team)\n",
    "\n",
    "url_batsman = 'https://icc-cricket.com'\n",
    "res_1 = requests.get(url_batsman)\n",
    "print(r)\n",
    "soup = BeautifulSoup(r.content)\n",
    "batsmans = []\n",
    "for team_row in soup.find_all(\"div\", class_ = 'si-table-section'):\n",
    "    name = team_row.find('div', class_ = 'si-table-data si-team').text\n",
    "    matches = team_row.find_all('div', class_ = \"si-table-data si-matches\")\n",
    "    points = team_row.find_all('div' ,class_ = 'si-table-data si-pts')\n",
    "    rating = team_row.find_all('div',class_ = \"si-table-data si-rating\")\n",
    "    batsmans.append({'name': name, 'matches': matches, 'points': points, 'rating': rating})\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "for batsman in batsmans:\n",
    "    print(batsman)\n",
    "\n",
    "url_bowlers = 'https://icc-cricket.com'\n",
    "res_2 = requests.get(url_bowlers)\n",
    "print(r)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "bowlers = []\n",
    "for team_row in soup.find_all(\"div\", class_ = 'si-table-section'):\n",
    "    name = team_row.find('div', class_ = 'si-table-data si-team')\n",
    "    matches = team_row.find_all('div', class_ = \"si-table-data si-matches\")\n",
    "    points = team_row.find_all('div' ,class_ = 'si-table-data si-pts')\n",
    "    rating = team_row.find_all('div',class_ = \"si-table-data si-rating\")\n",
    "    bowlers.append({'name': name, 'matches': matches, 'points': points, 'rating': rating})\n",
    "\n",
    "print(\"Top 10 ODI Bowlers:\")\n",
    "for bowler in bowlers:\n",
    "    print(bowler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3625b82c-08ad-4cf0-8d1a-8bc56674e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "url_1 = 'https://www.patreon.com/posts/python-tutorial-77834814'\n",
    "\n",
    "response = requests.get(url_1)\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e75abb05-e055-4390-8c02-0485fcaa83c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'house_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     emi \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont-semi-bold\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     13\u001b[0m     price \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading-6\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(house_title)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'house_title' is not defined"
     ]
    }
   ],
   "source": [
    "localities = [\"indira-nagar\", \"jayanagar\", \"rajaji-nagar\"]\n",
    "\n",
    "for locality in localities:\n",
    "    url = f\"https://www.nobroker.in/property/sale/bangalore/{locality}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "for details in soup.find_all('div', class_=''):\n",
    "    house_title = details.soup\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f182f717-59ba-4746-b67d-9877a88d7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "product_containers = soup.find_all('div', class_='productCardBox   ')\n",
    "\n",
    "for index, container in enumerate(product_containers[:10], start=1):\n",
    "    product_name = container.find('h2', class_='clr-shade4 h3-p-name   undefined false  ').text.strip()\n",
    "    product_price = container.find('div', class_='discountedPriceText clr-p-black   false  ').text.strip()\n",
    "    product_image_url = container.find('productImg')['src']\n",
    "    \n",
    "    # Print product details\n",
    "    print(f\"Product {index}:\")\n",
    "    print(f\"Name: {product_name}\")\n",
    "    print(f\"Price: {product_price}\")\n",
    "    print(f\"Image URL: {product_image_url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "49a98766-3c07-4604-8a00-f5e6abb41eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cnbc.com/technology/'\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "news_links = []\n",
    "\n",
    "articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "for article in articles:\n",
    "    heading = article.find('a', class_='Card-title')\n",
    "    date = article.find('time', class_='Card-time')\n",
    "    link = heading['href'] if heading else None\n",
    "\n",
    "    headings.append(heading.text.strip() if heading else None)\n",
    "    dates.append(date.text.strip() if date else None)\n",
    "    news_links.append(link)\n",
    "\n",
    "for i in range(len(headings)):\n",
    "    print(heading)\n",
    "    print(date)\n",
    "    print(link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbb2e1-c25c-486f-ad38-d55ee48a9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8) sir last site is not opening/not working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
